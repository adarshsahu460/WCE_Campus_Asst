# WCE Campus Assistant

A production-ready AI-powered Campus Assistant Chatbot for Walchand College of Engineering (WCE). The system combines a Retrieval-Augmented Generation (RAG) pipeline with a Model Context Protocol (MCP) server to provide intelligent responses to student queries about timetables, exams, syllabi, and more.

## ğŸš€ Features

- **RAG-Powered Responses**: Answers questions based on college documents (syllabi, notices, regulations)
- **MCP Tools**: Four specialized tools for common student queries
  - ğŸ“… **Timetable Tool**: Get class schedules for any day
  - ğŸ“š **Study Plan Tool**: Generate personalized study schedules
  - ğŸ”” **Exam Notify Tool**: Get upcoming exam reminders
  - ğŸ“ **File Browser Tool**: Browse available documents
- **Document Upload**: Upload and index new documents on the fly
- **Modern UI**: Clean, responsive chat interface with markdown support

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js   â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚  ChromaDB   â”‚
â”‚   Frontend  â”‚     â”‚   Backend    â”‚     â”‚  VectorDB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  MCP Server  â”‚
                    â”‚  (4 Tools)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- Groq API key (for LLM chat - get free at https://console.groq.com)
- Node.js 20+ (for local development)
- Python 3.11+ (for local development)

## ğŸš€ Quick Start

### Using Docker Compose (Recommended)

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-org/wce-assistant.git
   cd wce-assistant
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env and add your Groq API key
   ```

3. **Start all services**
   ```bash
   docker-compose up -d
   ```

4. **Seed the database**
   ```bash
   docker-compose exec backend python -m api.rag.seed
   ```

5. **Access the application**
   - Frontend: http://localhost:3000
   - Backend API: http://localhost:8001/docs
   - MCP Server: http://localhost:8002/docs

### Local Development

#### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start ChromaDB (in separate terminal)
docker run -p 8000:8000 chromadb/chroma

# Start backend API
uvicorn api.main:app --host 0.0.0.0 --port 8001 --reload

# Start MCP server (in separate terminal)
cd mcp-server
uvicorn server:app --host 0.0.0.0 --port 8002 --reload
```

#### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

## ğŸ“ Project Structure

```
wce-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ loader.py        # Document loader
â”‚   â”‚   â”‚   â”œâ”€â”€ splitter.py      # Text splitter
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py    # Embedding models
â”‚   â”‚   â”‚   â”œâ”€â”€ vectordb.py      # ChromaDB wrapper
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py     # Document retriever
â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py      # RAG orchestration
â”‚   â”‚   â”‚   â””â”€â”€ seed.py          # Database seeding
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ chat.py          # Chat endpoint
â”‚   â”‚       â””â”€â”€ upload.py        # Upload endpoint
â”‚   â”œâ”€â”€ mcp-server/
â”‚   â”‚   â”œâ”€â”€ server.py            # MCP server
â”‚   â”‚   â””â”€â”€ tools/               # MCP tools
â”‚   â”œâ”€â”€ tests/                   # Unit tests
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx             # Home page
â”‚   â”‚   â”œâ”€â”€ chat/page.tsx        # Chat page
â”‚   â”‚   â”œâ”€â”€ components/          # React components
â”‚   â”‚   â””â”€â”€ api/chat/route.ts    # API proxy
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ data/                        # Document storage
â”‚   â”œâ”€â”€ syllabus/
â”‚   â”œâ”€â”€ notices/
â”‚   â”œâ”€â”€ regulations/
â”‚   â”œâ”€â”€ timetables/
â”‚   â””â”€â”€ exams/
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ mcptools.md
â”‚   â”œâ”€â”€ rag_pipeline.md
â”‚   â””â”€â”€ api_reference.md
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `GROQ_API_KEY` | Groq API key for LLM | Required |
| `LLM_MODEL` | Groq model name | llama-3.3-70b-versatile |
| `CHROMA_HOST` | ChromaDB host | localhost |
| `CHROMA_PORT` | ChromaDB port | 8000 |
| `EMBEDDING_MODEL` | HuggingFace embedding model | BAAI/bge-large-en-v1.5 |
| `MCP_SERVER_URL` | MCP server URL | http://localhost:8002 |
| `DATA_DIR` | Data directory | ./data |

### RAG Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| Chunk Size | 1000 | Characters per chunk |
| Chunk Overlap | 200 | Overlap between chunks |
| Top K | 5 | Retrieved documents |
| Score Threshold | 0.35 | Minimum similarity |

## ğŸ“š API Reference

### Chat Endpoint

```bash
POST /chat
Content-Type: application/json

{
  "message": "What classes do I have today?",
  "context": {
    "class": "TE",
    "division": "A"
  }
}
```

### Upload Endpoint

```bash
POST /upload
Content-Type: multipart/form-data

file: <document>
category: syllabus
```

### RAG Query

```bash
POST /rag/query
Content-Type: application/json

{
  "query": "Machine Learning syllabus",
  "top_k": 5
}
```

See [API Reference](docs/api_reference.md) for complete documentation.

## ğŸ§ª Testing

```bash
cd backend

# Run all tests
pytest

# Run with coverage
pytest --cov=api --cov-report=html

# Run specific test file
pytest tests/test_rag.py -v
```

## ğŸ“– Example Queries

| Query | Tool/RAG |
|-------|----------|
| "What classes do I have today?" | Timetable Tool |
| "Show me the ML syllabus" | RAG |
| "Create a study plan for my exams" | Study Plan Tool |
| "When is my next exam?" | Exam Notify Tool |
| "What documents are available?" | File Browser Tool |
| "What are the attendance rules?" | RAG |

## ğŸ³ Docker Commands

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f backend

# Rebuild containers
docker-compose build

# Stop all services
docker-compose down

# Remove volumes (reset data)
docker-compose down -v
```

## ğŸ“Š Monitoring

- Health check: `GET /health`
- Readiness: `GET /ready`
- API docs: `http://localhost:8001/docs`
- MCP docs: `http://localhost:8002/docs`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Walchand College of Engineering for the use case
- Groq for fast LLM inference
- HuggingFace for embedding models
- ChromaDB for vector storage
- FastAPI and Next.js communities

---

Built with â¤ï¸ for WCE students
